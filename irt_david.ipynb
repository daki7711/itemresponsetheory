{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import range\n",
    "from numpy import (abs, array, concatenate, copy, exp, inf, log, log1p, max,\n",
    "                   newaxis, sort, sum)\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import dirichlet, lognorm, norm\n",
    "from scipy.special import expit\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale parameters for the a-priory distribution of parameters -\n",
    "# The standard deviation of theta (student ability)\n",
    "THETA_SCALE = 1.\n",
    "# The standard deviation of beta (item difficulty)\n",
    "BETA_SCALE = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters are initialized as the avg. of a large number of parameters\n",
    "# which are randomly generated, how large should this number be?\n",
    "INITIAL_PARAMETER_AVG = 100\n",
    "\n",
    "# When scipy.optimize minimization fails with these messages, we can\n",
    "# expect the result reached to still be pretty good\n",
    "OPTIMIZE_MAX_REACHED_MSG = ['Maximum number of iterations has been exceeded.',\n",
    "                            'Maximum number of function evaluations '\n",
    "                            'has been exceeded.']\n",
    "# Method parameter for scipy.optimize.minimize\n",
    "MINIMIZATION_METHOD = 'Nelder-Mead'\n",
    "\n",
    "# Bound on the number of MLE iterations\n",
    "MAX_ITER = 100\n",
    "# If no parameters has changed by more than DIFF in the last SMALL_DIFF_STREAK\n",
    "# iterations, then we terminate the optimization process\n",
    "DIFF = 0.001\n",
    "SMALL_DIFF_STREAK = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(a):\n",
    "    return expit(a)\n",
    "    #return 1./(1+np.exp(-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_irt_model(beta, theta):\n",
    "    return expit(theta - beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentParametersDistribution(object):\n",
    "    \"\"\"\n",
    "    An object for generation and calculation of student parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, theta_scale=THETA_SCALE):\n",
    "        self.theta = norm(loc=0., scale=theta_scale)\n",
    "\n",
    "    def rvs(self, size=None):\n",
    "        return self.theta.rvs(size)\n",
    "\n",
    "    def logpdf(self, theta):\n",
    "        return self.theta.logpdf(theta)\n",
    "\n",
    "\n",
    "class QuestionParametersDistribution(object):\n",
    "    \"\"\"\n",
    "    An object for generation and calculation of question parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, beta_scale=BETA_SCALE):\n",
    "        self.beta = norm(loc=0., scale=beta_scale)\n",
    "\n",
    "    def rvs(self, size=None):\n",
    "        return self.beta.rvs(size)\n",
    "\n",
    "    def logpdf(self, beta):\n",
    "        return self.beta.logpdf(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_random_values(students_count, subquestions_count,\n",
    "                             student_dist, question_dist):\n",
    "    theta_values = sum(student_dist.rvs(size=(INITIAL_PARAMETER_AVG,\n",
    "                                              students_count)),\n",
    "                       axis=0) / INITIAL_PARAMETER_AVG\n",
    "    beta_values = sum(question_dist.rvs(size=(INITIAL_PARAMETER_AVG,\n",
    "                                              subquestions_count)),\n",
    "                      axis=0) / INITIAL_PARAMETER_AVG\n",
    "    return theta_values, beta_values\n",
    "\n",
    "## What is this doing?????\n",
    "# This is were we handle continuous variables so we need to look more here for continuous values\n",
    "def expanded_scores(score_matrix):\n",
    "    answers = [list(set(score)) for score in score_matrix]\n",
    "    subscores = []\n",
    "    for i, question_scores in enumerate(score_matrix):\n",
    "        best = len(answers[i])\n",
    "        subscores_per_student = []\n",
    "        for student_score in question_scores:\n",
    "            expanded = [1] * answers[i].index(student_score)\n",
    "            if len(expanded) < best - 1:\n",
    "                expanded = expanded + [-1]\n",
    "                expanded = expanded + [0] * (best - 1 - len(expanded))\n",
    "            subscores_per_student.append(expanded)\n",
    "        subscores.append(array(subscores_per_student).T)\n",
    "    return concatenate(subscores)\n",
    "\n",
    "def initialize_estimation(scores, student_dist, question_dist):\n",
    "    # Even though we usually input the table as scores per student,\n",
    "    # the analysis is easier for a table of scores per question:\n",
    "    scores = scores.T\n",
    "    questions_count, students_count = scores.shape\n",
    "    # Split each question into small sub-question.\n",
    "    \n",
    "    # This part gets the different answers for a certain question\n",
    "    answers_per_question = [sort(array(list(set(score)))) for score in scores]\n",
    "    # This part counts how many different answer there are for a certain question\n",
    "    subquestions_per_question = [len(answers) for answers in answers_per_question]\n",
    "    # This part counts the total number of subquestions\n",
    "    subquestions_count = sum(subquestions_per_question) - questions_count\n",
    "    # Begin with small random values per parameter to break symmetry.\n",
    "    thetas, betas = initialize_random_values(students_count,\n",
    "                                             subquestions_count,\n",
    "                                             student_dist, question_dist)\n",
    "    # Modify the question array according to those new sub-questions.\n",
    "    expanded = expanded_scores(scores)\n",
    "    return expanded, thetas, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_optimization_result(res):\n",
    "    \"\"\"\n",
    "    Modify the dictionary returned by scipy.optimize.minimize to get the\n",
    "    actual optimal value of `x` obtained by the minimization process.\n",
    "    \"\"\"\n",
    "    if not res['success'] and res['message'] not in OPTIMIZE_MAX_REACHED_MSG:\n",
    "        raise RuntimeError(\"Optimization failed:\\n\" + repr(res))\n",
    "    return res['x']\n",
    "\n",
    "\n",
    "def learn_beta(thetas, question_dist, corrects):\n",
    "    \"\"\"\n",
    "    Returns a function that can calculate the log-likelihood of question\n",
    "    parameters given thetas and answers of different students.\n",
    "\n",
    "    Returns an estimation function suitable for optimization.\n",
    "    \"\"\"\n",
    "    def f(arg):\n",
    "        b = arg\n",
    "        mult = question_dist.logpdf(b)\n",
    "        p = base_irt_model(b, thetas)\n",
    "        for i, correct in enumerate(corrects):\n",
    "            # correct answer is 1, incorrect by -1, no answer is 0\n",
    "            # multiply by a factor of 2 per student, to utilize log1p\n",
    "            mult += log1p((2 * p[i] - 1) * correct)\n",
    "        return -mult\n",
    "    return f\n",
    "\n",
    "def question_beta_given_theta(thetas, question_dist, scores, initial_beta):\n",
    "    \"\"\"\n",
    "    find the maximal-likelihood question parameters, given the ability\n",
    "    and answers of all students, for a single question\n",
    "    \"\"\"\n",
    "    to_minimize = learn_beta(thetas, question_dist, scores)\n",
    "    res = minimize(to_minimize, initial_beta, method=MINIMIZATION_METHOD)\n",
    "    return parse_optimization_result(res)\n",
    "\n",
    "\n",
    "def all_betas_given_theta(thetas, question_dist, scores, betas):\n",
    "    \"\"\"\n",
    "    find the maximal-likelihood question parameters, given the ability\n",
    "    and answers of all students, for all questions question\n",
    "    \"\"\"\n",
    "    return array([question_beta_given_theta(thetas, question_dist, scores[i], betas[i])\n",
    "                  for i in range(len(betas))])\n",
    "\n",
    "def learn_theta(betas, student_dist, corrects):\n",
    "    \"\"\"\n",
    "    Returns a function that can calculate the log-likelihood of student\n",
    "    parameters given question parameters and answers of different\n",
    "    students.\n",
    "\n",
    "    Returns an estimation function suitable for optimization.\n",
    "    \"\"\"\n",
    "    def f(theta):\n",
    "        theta = theta[0]\n",
    "        mult = student_dist.logpdf(theta)\n",
    "        b = betas\n",
    "        p = base_irt_model(b, theta)\n",
    "        for i, correct in enumerate(corrects):\n",
    "            mult += log1p((2 * p[i] - 1) * correct)\n",
    "        return -mult\n",
    "    return f\n",
    "\n",
    "def student_theta_given_beta(betas, student_dist, scores, inital_theta):\n",
    "    \"\"\"\n",
    "    find the maximal-likelihood ability parameter of a single student,\n",
    "    given his answers and the parameters of the questions\n",
    "    \"\"\"\n",
    "    to_minimize = learn_theta(betas, student_dist, scores)\n",
    "    res = minimize(to_minimize, [inital_theta], method=MINIMIZATION_METHOD)\n",
    "    return parse_optimization_result(res)\n",
    "\n",
    "\n",
    "def all_thetas_given_beta(betas, student_dist, scores, thetas):\n",
    "    \"\"\"\n",
    "    find the maximal-likelihood ability parameter of a all students,\n",
    "    given their answers and the parameters of the questions\n",
    "    \"\"\"\n",
    "    return array([student_theta_given_beta(betas, student_dist, scores[:, i], thetas[i])\n",
    "                  for i in range(len(thetas))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_thetas(scores, verbose=False):\n",
    "    \"\"\"\n",
    "    Estimates the student theta (ability) and question parameters.\n",
    "\n",
    "    Currently uses JMLE to simultaneously estimate the parameters for\n",
    "    students and for questions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scores : array_like\n",
    "        A 2-dimensional array of question scores, where each row\n",
    "        corresponds to a single student. Grades should be integers but\n",
    "        their scale can be arbitrary (supports partial credit, not only\n",
    "        0 and 1).\n",
    "\n",
    "    \"\"\"\n",
    "    student_dist = StudentParametersDistribution()\n",
    "    question_dist = QuestionParametersDistribution()\n",
    "    expanded, thetas, betas = initialize_estimation(scores, student_dist, question_dist)\n",
    "    small_diffs_streak = 0\n",
    "    iter_count = 0\n",
    "    while iter_count < MAX_ITER and small_diffs_streak < SMALL_DIFF_STREAK:\n",
    "        old_betas, old_thetas = copy(betas), copy(thetas)\n",
    "        betas = all_betas_given_theta(thetas, question_dist, expanded, betas)\n",
    "        thetas = all_thetas_given_beta(betas, student_dist, expanded, thetas)\n",
    "        # How much have the parameters changed from last time?\n",
    "        diff = max([max(abs(old_betas - betas)), max(abs(old_thetas - thetas))])\n",
    "        if diff < DIFF:\n",
    "            small_diffs_streak += 1\n",
    "        else:\n",
    "            small_diffs_streak = 0\n",
    "        iter_count += 1\n",
    "        #if verbose:\n",
    "        #    print(diff)\n",
    "    return thetas, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = []\n",
    "acc = []\n",
    "theta_spearman = []\n",
    "diff_spearman = []\n",
    "\n",
    "theta_pearson = []\n",
    "diff_pearson = []\n",
    "\n",
    "for i in range(20,500,20):\n",
    "    size.append(i)\n",
    "    n_s = i # number of students\n",
    "    n_i = i # number of items\n",
    "\n",
    "    # pick a random ability for each student from an N(0,1) distribution\n",
    "\n",
    "    abilities = np.random.randn(n_s,1)\n",
    "\n",
    "    # pick a random difficulty for each item from an N(0,1) distribution\n",
    "\n",
    "    difficulties = np.random.randn(1,n_i)\n",
    "\n",
    "    # the IRT model says that P(correct[s,i]) = logistic(ability[s] -difficulty[i])\n",
    "    prob_correct = logistic(abilities - difficulties) \n",
    "\n",
    "    # flip a coin to pick 'correct' or 'incorrect' for each student based on the\n",
    "    # probability of a correct response\n",
    "\n",
    "    student_responses = np.random.binomial(1,prob_correct)\n",
    "    \n",
    "    # This is the part where the student ability and the item difficulty is being trained\n",
    "    # This estimation uses the four paremeter model\n",
    "    thetas, betas = estimate_thetas(student_responses)\n",
    "\n",
    "    # This is where we restore the responses with the obtained parameters\n",
    "    prob_correct = base_irt_model(betas.T, thetas)\n",
    "\n",
    "    prediction = np.random.binomial(1,prob_correct)\n",
    "    for s in range(n_s):\n",
    "        for i in range(n_i):\n",
    "            if prob_correct[s][i] > 0.5:\n",
    "                prediction[s][i] = 1\n",
    "            else:\n",
    "                prediction[s][i] = 0\n",
    "    \n",
    "    # This is where we test the restored responses again\n",
    "    total_number_of_responses = n_s * n_i\n",
    "    correct = 0\n",
    "    for s in range(n_s):\n",
    "        for i in range(n_i):\n",
    "            if student_responses[s][i] == prediction[s][i]:\n",
    "                correct += 1\n",
    "    acc.append(correct/total_number_of_responses)\n",
    "    \n",
    "    correlation, p_value = spearmanr(abilities,thetas)\n",
    "    theta_spearman.append(correlation)\n",
    "\n",
    "    correlation, p_value = spearmanr(difficulties.flatten(),betas)\n",
    "    diff_spearman.append(correlation)\n",
    "\n",
    "    correlation, p_value = pearsonr(abilities.flatten(), thetas.flatten())\n",
    "    theta_pearson.append(correlation)\n",
    "\n",
    "    correlation, p_value = pearsonr(difficulties.flatten(),betas.flatten().T)\n",
    "    diff_pearson.append(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('The accuracy plot')\n",
    "plt.xlabel('size of the number of students and items')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(size, acc,'ro')\n",
    "plt.savefig('the_accuracy_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Spearman Rank for Student ability')\n",
    "plt.xlabel('size of the number of students and items')\n",
    "plt.ylabel('Spearman Rank correlation')\n",
    "plt.plot(size, theta_spearman,'bo')\n",
    "plt.savefig('Spearman Rank for Student ability.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Pearson Correlation for Student ability')\n",
    "plt.xlabel('size of the number of students and items')\n",
    "plt.ylabel('Pearson Correlation Value')\n",
    "plt.plot(size, theta_pearson,'bo')\n",
    "plt.savefig('Pearson Correlation for Student ability.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Spearman Rank for Item Difficulty')\n",
    "plt.xlabel('size of the number of students and items')\n",
    "plt.ylabel('Spearman Rank correlation')\n",
    "plt.plot(size, diff_spearman,'go')\n",
    "plt.savefig('Spearman Rank for Item Difficulty.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Pearson Correlation for Item Difficulty')\n",
    "plt.xlabel('size of the number of students and items')\n",
    "plt.ylabel('Pearson Correlation Value')\n",
    "plt.plot(size, diff_pearson,'go')\n",
    "plt.savefig('Pearson Correlation for Item Difficulty.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
